# ğŸ›’ Amazon Review Sentiment Analyzer

![Build](https://github.com/Gabxrielle/amazon-review-sentiment-analyzer/actions/workflows/ci.yml/badge.svg)
![Python](https://img.shields.io/badge/python-3.10%7C3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)

Baseline sentiment analysis on Amazon product reviews using **TF-IDF + Logistic Regression**.  
The goal is to build a clear, modular, and reproducible NLP pipeline for text classification (negative, neutral, positive).

---

## ğŸ“Œ Context

The project was originally designed to run on the **Amazon Reviews** dataset (`amazon_reviews_us_Beauty_v1_00.tsv.gz`).  
However, public Amazon datasets were removed from Hugging Face in 2024.  

â¡ï¸ **To avoid blocking development**, this M1 uses the **Yelp Review Full** dataset (1â€“5 star ratings) as a *fallback*.  
The pipeline remains fully **compatible with Amazon TSV/TSV.GZ dumps**: just set `DATA_SRC` to your local Amazon file and rerun `src/data.py`.

---

## ğŸ“‚ Project Structure

```

amazon-review-sentiment-analyzer/
â”œâ”€â”€ data/                  # train/val/test.parquet
â”œâ”€â”€ metrics/               # evaluation reports
â”‚   â”œâ”€â”€ baseline\_val.json
â”‚   â””â”€â”€ baseline\_test.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ baseline/tfidf\_lr.joblib
â”œâ”€â”€ reports/               # visuals and reports
â”‚   â”œâ”€â”€ confusion\_matrix\_test.png
â”‚   â”œâ”€â”€ classification\_report\_test.json
â”‚   â”œâ”€â”€ classification\_report\_test.csv
â”‚   â””â”€â”€ misclassified\_test.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py            # data preparation
â”‚   â”œâ”€â”€ train\_sklearn.py   # TF-IDF + Logistic Regression baseline
â”‚   â”œâ”€â”€ train\_hf.py        # (M2) Hugging Face Transformers baseline
â”‚   â””â”€â”€ eval\_hf.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements-m1.txt

````

---

## âš™ï¸ Usage

### 1. Prepare data
With Yelp fallback (default 100k rows):
```bash
export YELP_ROWS=100000
python src/data.py
````

With Amazon TSV/GZ (if you have the local dump):

```bash
export DATA_SRC=data/raw/amazon_reviews_us_Beauty_v1_00.tsv.gz
python src/data.py
```

This generates:

```
data/train.parquet
data/val.parquet
data/test.parquet
```

---

### 2. Train baseline

```bash
python src/train_sklearn.py
```

This saves:

```
models/baseline/tfidf_lr.joblib
metrics/baseline_val.json
metrics/baseline_test.json
```

---

### 3. Generate visuals

```bash
python src/report_baseline.py
```

This saves:

```
reports/confusion_matrix_test.png
reports/classification_report_test.json
reports/classification_report_test.csv
reports/misclassified_test.csv
```

---

## âš¡ Makefile quickstart

```bash
make prepare   # generate data splits
make train     # train TF-IDF + LR and save metrics/model
make report    # generate confusion matrix, reports, top n-grams
make infer     # quick prediction demo
```

---

## ğŸ“Š Baseline Results (M1)

Example run with a small Yelp subset:

| Split | Accuracy | Notes                                |
| ----- | -------- | ------------------------------------ |
| Val   | 0.70     | Neutral class missing (small sample) |
| Test  | 0.80     | Unstable with few examples           |

**Validation**

```
neg:  precision=0.75  recall=1.00  f1=0.83
neu:  no examples in this sample
pos:  precision=0.67  recall=0.80  f1=0.73
```

**Test**

```
neg:  precision=0.71  recall=1.00  f1=0.83
neu:  no examples in this sample
pos:  precision=1.00  recall=0.75  f1=0.86
```

ğŸ“Œ Note: results are unstable with very small samples. With larger datasets (50kâ€“100k+ reviews), metrics stabilize.

---

## ğŸ” Baseline Visuals

Example confusion matrix on the test set:

![Confusion Matrix](reports/confusion_matrix_test.png)

Additional reports:

* [Classification Report (CSV)](reports/classification_report_test.csv)
* [Misclassified Examples (CSV)](reports/misclassified_test.csv)

---

## ğŸš€ Roadmap

* **M2:** Train a baseline Transformer model (e.g. `distilbert-base-uncased`) with Hugging Face (`src/train_hf.py`).
* **M3:** Compare baseline TF-IDF+LR vs Transformers.
* **M4:** Serve the model via API (FastAPI/Flask).
* **M5:** Build a dashboard for metrics and visualizations.
* **M6:** Add automated tests + CI with GitHub Actions.
* **M7:** Containerize with Docker for reproducibility.

---

## âš ï¸ Limitations

* Neutral class is unstable with small subsets.
* Yelp reviews differ from Amazon domain (domain shift).
* Sarcasm and figurative language are not well captured by linear models.
* Baseline metrics can vary significantly depending on dataset size.

---

## ğŸ“– Notes

* Compatible with **Amazon Reviews TSV/GZ dumps** (if available).
* Yelp is used **only as a temporary fallback** to validate the pipeline.
* The modular design ensures data sources can be swapped without changing the training/evaluation code.

---

## ğŸ‘©â€ğŸ’» Author

Developed by **Gabrielle Rodrigues**
[LinkedIn](https://www.linkedin.com/in/grdrgs/) | [GitHub](https://github.com/Gabxrielle)

```