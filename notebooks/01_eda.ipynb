{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Amazon Reviews — EDA (neg / neu / pos)\n",
          "\n",
          "Exploratory Data Analysis for the sentiment dataset used in this project.\n",
          "\n",
          "**What you get here**\n",
          "- Sanity checks (versions, device)\n",
          "- Load `train/val/test.parquet`\n",
          "- Label distribution (per split and overall)\n",
          "- Text length analysis (words/chars) with plots\n",
          "- Basic data quality checks (empties/duplicates)\n",
          "\n",
          "> ⚠️ **Prereq:** run `python src/data.py` first so that `data/train.parquet`, `data/val.parquet`, and `data/test.parquet` exist.\n"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Imports & versions\n",
          "from pathlib import Path\n",
          "import os, json, math, random\n",
          "import numpy as np\n",
          "import pandas as pd\n",
          "import matplotlib.pyplot as plt\n",
          "from collections import Counter\n",
          "from IPython.display import display, Markdown\n",
          "\n",
          "try:\n",
          "    import torch\n",
          "    mps_ok = getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available()\n",
          "except Exception:\n",
          "    torch = None\n",
          "    mps_ok = False\n",
          "\n",
          "print(\"Python:\", f\"{pd.util.version.get_version()} (pandas)\")\n",
          "print(\"pandas:\", pd.__version__)\n",
          "print(\"numpy:\", np.__version__)\n",
          "if torch is not None:\n",
          "    print(\"torch:\", torch.__version__, \"| MPS available:\", mps_ok)\n",
          "else:\n",
          "    print(\"torch: not installed in this kernel\")\n",
          "\n",
          "# Matplotlib defaults\n",
          "plt.rcParams[\"figure.figsize\"] = (7.0, 4.0)\n",
          "plt.rcParams[\"axes.grid\"] = True\n",
          "plt.rcParams[\"axes.spines.top\"] = False\n",
          "plt.rcParams[\"axes.spines.right\"] = False\n"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Paths & presence checks\n",
          "ROOT = Path(\"..\").resolve()\n",
          "DATA_DIR = ROOT / \"data\"\n",
          "REPORT_DIR = ROOT / \"reports\" / \"figures\"\n",
          "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
          "\n",
          "required = [DATA_DIR/\"train.parquet\", DATA_DIR/\"val.parquet\", DATA_DIR/\"test.parquet\"]\n",
          "missing = [p for p in required if not p.exists()]\n",
          "if missing:\n",
          "    print(\"❌ Missing files:\")\n",
          "    for p in missing:\n",
          "        print(\" -\", p)\n",
          "    print(\"\\nRun first:  python src/data.py\")\n",
          "    raise SystemExit(0)\n",
          "else:\n",
          "    print(\"✅ Parquet files found.\")\n"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Helpers to locate text/label columns across variants\n",
          "TEXT_CANDS = [\"text\", \"reviewText\", \"review_body\", \"review_text\", \"summary\"]\n",
          "\n", 
          "def get_text_col(df):\n",
          "    for c in TEXT_CANDS:\n",
          "        if c in df.columns:\n",
          "            return c\n",
          "    # fallback: first string-like column\n",
          "    for c in df.columns:\n",
          "        if pd.api.types.is_string_dtype(df[c]):\n",
          "            return c\n",
          "    raise KeyError(\"Couldn't find a text column. Expected one of: \" + \", \".join(TEXT_CANDS))\n",
          "\n",
          "def get_label_series(df):\n",
          "    if \"label_str\" in df.columns:\n",
          "        return df[\"label_str\"]\n",
          "    if \"sentiment\" in df.columns:\n",
          "        return df[\"sentiment\"]\n",
          "    if \"label\" in df.columns:\n",
          "        id2lbl = {0:\"neg\", 1:\"neu\", 2:\"pos\"}\n",
          "        return df[\"label\"].map(id2lbl).fillna(df[\"label\"].astype(str))\n",
          "    raise KeyError(\"Couldn't find a label column (label, label_str, or sentiment).\")\n",
          "\n",
          "def read_split(name):\n",
          "    path = DATA_DIR / f\"{name}.parquet\"\n",
          "    return pd.read_parquet(path)\n",
          "\n",
          "splits = {s: read_split(s) for s in (\"train\",\"val\",\"test\")}\n",
          "for k, df in splits.items():\n",
          "    print(k.upper(), df.shape)\n",
          "    display(df.head(3))\n",
          "\n",
          "TEXT_COL = get_text_col(splits[\"train\"])  # consistent across splits\n",
          "print(\"\\nUsing text column:\", TEXT_COL)\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Label distribution\n",
          "Per split and combined. Labels expected: `neg`, `neu`, `pos`."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def label_counts(df):\n",
          "    s = get_label_series(df)\n",
          "    vc = s.value_counts().reindex([\"neg\",\"neu\",\"pos\"], fill_value=0)\n",
          "    return vc\n",
          "\n",
          "dist_tbl = []\n",
          "for name, df in splits.items():\n",
          "    vc = label_counts(df)\n",
          "    total = int(vc.sum())\n",
          "    dist_tbl.append(pd.DataFrame({\n",
          "        \"split\": name,\n",
          "        \"neg\": [vc.get(\"neg\",0)],\n",
          "        \"neu\": [vc.get(\"neu\",0)],\n",
          "        \"pos\": [vc.get(\"pos\",0)],\n",
          "        \"total\": [total],\n",
          "        \"neg_%\": [vc.get(\"neg\",0)/total*100],\n",
          "        \"neu_%\": [vc.get(\"neu\",0)/total*100],\n",
          "        \"pos_%\": [vc.get(\"pos\",0)/total*100],\n",
          "    }))\n",
          "\n",
          "all_df = pd.concat([splits[\"train\"], splits[\"val\"], splits[\"test\"]], ignore_index=True)\n",
          "vc_all = label_counts(all_df)\n",
          "total_all = int(vc_all.sum())\n",
          "dist_tbl.append(pd.DataFrame({\n",
          "    \"split\": [\"all\"],\n",
          "    \"neg\": [vc_all.get(\"neg\",0)],\n",
          "    \"neu\": [vc_all.get(\"neu\",0)],\n",
          "    \"pos\": [vc_all.get(\"pos\",0)],\n",
          "    \"total\": [total_all],\n",
          "    \"neg_%\": [vc_all.get(\"neg\",0)/total_all*100],\n",
          "    \"neu_%\": [vc_all.get(\"neu\",0)/total_all*100],\n",
          "    \"pos_%\": [vc_all.get(\"pos\",0)/total_all*100],\n",
          "}))\n",
          "\n",
          "dist = pd.concat(dist_tbl, ignore_index=True)\n",
          "dist = dist[[\"split\",\"total\",\"neg\",\"neu\",\"pos\",\"neg_%\",\"neu_%\",\"pos_%\"]]\n",
          "display(dist)\n",
          "\n",
          "# Bar plot per split\n",
          "fig, ax = plt.subplots(figsize=(7,4))\n",
          "X = np.arange(len(splits))\n",
          "w = 0.25\n",
          "neg = [label_counts(splits[s]).get(\"neg\",0) for s in (\"train\",\"val\",\"test\")]\n",
          "neu = [label_counts(splits[s]).get(\"neu\",0) for s in (\"train\",\"val\",\"test\")]\n",
          "pos = [label_counts(splits[s]).get(\"pos\",0) for s in (\"train\",\"val\",\"test\")]\n",
          "ax.bar(X - w, neg, width=w, label=\"neg\")\n",
          "ax.bar(X + 0.0, neu, width=w, label=\"neu\")\n",
          "ax.bar(X + w, pos, width=w, label=\"pos\")\n",
          "ax.set_xticks(X, [\"train\",\"val\",\"test\"]) \n",
          "ax.set_ylabel(\"count\")\n",
          "ax.set_title(\"Label distribution per split\")\n",
          "ax.legend()\n",
          "plt.tight_layout()\n",
          "plt.savefig(REPORT_DIR/\"eda_label_distribution.png\", dpi=160)\n",
          "plt.show()\n",
          "print(\"Saved:\", REPORT_DIR/\"eda_label_distribution.png\")\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Text length analysis\n",
          "Histograms of word counts and char counts (per split & per class)."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def word_count(s):\n",
          "    if isinstance(s, str):\n",
          "        return len(s.split())\n",
          "    return 0\n",
          "\n",
          "def char_count(s):\n",
          "    if isinstance(s, str):\n",
          "        return len(s)\n",
          "    return 0\n",
          "\n",
          "for name, df in splits.items():\n",
          "    wc = df[TEXT_COL].map(word_count).clip(upper=512)\n",
          "    cc = df[TEXT_COL].map(char_count).clip(upper=3000)\n",
          "    fig, axes = plt.subplots(1,2, figsize=(10,3.8))\n",
          "    axes[0].hist(wc, bins=40, color=\"#4C78A8\")\n",
          "    axes[0].set_title(f\"{name} — words (clipped)\")\n",
          "    axes[0].set_xlabel(\"words\")\n",
          "    axes[0].set_ylabel(\"count\")\n",
          "    axes[1].hist(cc, bins=40, color=\"#F58518\")\n",
          "    axes[1].set_title(f\"{name} — chars (clipped)\")\n",
          "    axes[1].set_xlabel(\"chars\")\n",
          "    for ax in axes:\n",
          "        ax.grid(True, alpha=0.25)\n",
          "    plt.tight_layout()\n",
          "    out = REPORT_DIR / f\"eda_len_{name}.png\"\n",
          "    plt.savefig(out, dpi=160)\n",
          "    plt.show()\n",
          "    print(\"Saved:\", out)\n",
          "\n",
          "# Per-class word count on ALL\n",
          "all_df = pd.concat([splits[\"train\"], splits[\"val\"], splits[\"test\"]], ignore_index=True)\n",
          "all_df = all_df.copy()\n",
          "all_df[\"label_plot\"] = get_label_series(all_df)\n",
          "all_df[\"wc\"] = all_df[TEXT_COL].map(word_count).clip(upper=512)\n",
          "\n",
          "fig, ax = plt.subplots(figsize=(7,4))\n",
          "for lbl, color in zip([\"neg\",\"neu\",\"pos\"], [\"#4C78A8\", \"#54A24B\", \"#E45756\"]):\n",
          "    wc_lbl = all_df.loc[all_df[\"label_plot\"]==lbl, \"wc\"]\n",
          "    ax.hist(wc_lbl, bins=40, alpha=0.6, label=lbl, color=color)\n",
          "ax.set_title(\"Word count by class (clipped)\")\n",
          "ax.set_xlabel(\"words\")\n",
          "ax.set_ylabel(\"count\")\n",
          "ax.legend()\n",
          "plt.tight_layout()\n",
          "out = REPORT_DIR/\"eda_len_by_class.png\"\n",
          "plt.savefig(out, dpi=160)\n",
          "plt.show()\n",
          "print(\"Saved:\", out)\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Data quality checks\n",
          "- Empty texts\n",
          "- Duplicates (by text)\n",
          "- Basic length stats (words/chars)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "def is_empty(s):\n",
          "    if not isinstance(s, str):\n",
          "        return True\n",
          "    return len(s.strip()) == 0\n",
          "\n",
          "def quality_report(df, name):\n",
          "    text_col = TEXT_COL\n",
          "    n = len(df)\n",
          "    empties = df[text_col].map(is_empty).sum()\n",
          "    dups = df.duplicated(subset=[text_col]).sum()\n",
          "    wc = df[text_col].map(lambda x: len(str(x).split()))\n",
          "    cc = df[text_col].map(lambda x: len(str(x)))\n",
          "    rep = {\n",
          "        \"split\": name,\n",
          "        \"rows\": int(n),\n",
          "        \"empty_text\": int(empties),\n",
          "        \"duplicates_by_text\": int(dups),\n",
          "        \"wc_mean\": float(wc.mean()),\n",
          "        \"wc_p95\": float(wc.quantile(0.95)),\n",
          "        \"cc_mean\": float(cc.mean()),\n",
          "        \"cc_p95\": float(cc.quantile(0.95)),\n",
          "    }\n",
          "    return rep\n",
          "\n",
          "qr = pd.DataFrame([quality_report(splits[s], s) for s in (\"train\",\"val\",\"test\")])\n",
          "display(qr)\n",
          "\n",
          "with open(REPORT_DIR.parent/\"eda_quality.json\", \"w\") as f:\n",
          "    json.dump(qr.to_dict(orient=\"records\"), f, indent=2)\n",
          "print(\"Saved:\", REPORT_DIR.parent/\"eda_quality.json\")\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## (Optional) Peek metrics JSON already produced by scripts\n",
          "Loads `metrics/baseline_test.json` and `metrics/bert_test.json` if present to show test scores."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "METR_DIR = ROOT/\"metrics\"\n",
          "rows = []\n",
          "def pull(json_path, name):\n",
          "    try:\n",
          "        m = json.load(open(json_path))\n",
          "        acc = m.get(\"report\",{}).get(\"accuracy\", None)\n",
          "        f1m = m.get(\"report\",{}).get(\"macro avg\",{}).get(\"f1-score\", None)\n",
          "        return {\"model\": name, \"Test ACC\": acc, \"Test F1-macro\": f1m}\n",
          "    except Exception:\n",
          "        return None\n",
          "\n",
          "b = pull(METR_DIR/\"baseline_test.json\", \"TF-IDF + Logistic\")\n",
          "d = pull(METR_DIR/\"bert_test.json\", \"DistilBERT\")\n",
          "for r in (b,d):\n",
          "    if r: rows.append(r)\n",
          "\n",
          "if rows:\n",
          "    display(pd.DataFrame(rows))\n",
          "else:\n",
          "    print(\"No metrics JSON found yet — run training scripts to generate.\")\n"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3 (ipykernel)",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "name": "python",
        "version": "3.12"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 5
  }
  